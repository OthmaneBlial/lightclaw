#!/usr/bin/env python3
"""
LightClaw CLI

Commands:
- lightclaw onboard : Initialize runtime files in current directory.
- lightclaw run     : Run the bot using current directory as runtime home.
- lightclaw chat    : Chat in terminal using the same brain as Telegram.
"""

from __future__ import annotations

import asyncio
import argparse
import getpass
import logging
import os
import re
import shlex
import shutil
import sys
from pathlib import Path

from dotenv import load_dotenv


PROJECT_ROOT = Path(__file__).resolve().parent
RUNTIME_DIRNAME = ".lightclaw"
RUNTIME_WORKSPACE = f"{RUNTIME_DIRNAME}/workspace"
RUNTIME_DB = f"{RUNTIME_DIRNAME}/lightclaw.db"
TEMPLATE_FILES = ("IDENTITY.md", "SOUL.md", "USER.md")
RUN_PROVIDER_KEY_ENV = {
    "openai": "OPENAI_API_KEY",
    "xai": "XAI_API_KEY",
    "claude": "ANTHROPIC_API_KEY",
    "gemini": "GEMINI_API_KEY",
    "deepseek": "DEEPSEEK_API_KEY",
    "zai": "ZAI_API_KEY",
}
RUN_PROVIDER_MODELS = {
    "openai": ["gpt-5.2", "gpt-5.2-mini"],
    "xai": ["grok-4-latest", "grok-4-fast-non-reasoning"],
    "claude": ["claude-opus-4-5", "claude-sonnet-4-5"],
    "gemini": ["gemini-3-flash-preview", "gemini-2.5-flash"],
    "deepseek": ["deepseek-chat", "deepseek-reasoner"],
    "zai": ["glm-5", "glm-4.7"],
}


def _resolve_home(path_arg: str | None) -> Path:
    if path_arg:
        return Path(path_arg).expanduser().resolve()
    return Path.cwd().resolve()


def _configured_run_providers() -> list[str]:
    configured: list[str] = []
    for provider, env_key in RUN_PROVIDER_KEY_ENV.items():
        if os.getenv(env_key, "").strip():
            configured.append(provider)
    return configured


def _pick_number(prompt: str, max_value: int, default_value: int) -> int:
    raw = input(prompt).strip()
    if not raw:
        return default_value
    try:
        value = int(raw)
    except ValueError:
        return default_value
    if 1 <= value <= max_value:
        return value
    return default_value


def _prompt_run_provider_model(current_provider: str, current_model: str) -> tuple[str, str]:
    if not sys.stdin.isatty():
        return "", ""

    available = _configured_run_providers()
    if not available:
        return "", ""

    if current_provider in available:
        suggested_model = current_model or RUN_PROVIDER_MODELS.get(
            current_provider, ["latest"]
        )[0]
        print("")
        answer = input(
            f"Continue with current provider/model ({current_provider} / {suggested_model})? [Y/n]: "
        ).strip().lower()
        if answer in {"", "y", "yes", "ok"}:
            print(f"Using {current_provider} ({suggested_model}) for this run.")
            print("Tip: for custom model IDs, set LLM_MODEL directly in .env.")
            return current_provider, suggested_model

    print("")
    if len(available) == 1:
        selected_provider = available[0]
        print(f"Provider for this run: {selected_provider}")
    else:
        print("Choose provider for this run:")
        for idx, provider in enumerate(available, start=1):
            marker = " (current)" if provider == current_provider else ""
            default_model = RUN_PROVIDER_MODELS.get(provider, ["latest"])[0]
            print(f"  {idx}) {provider} ({default_model}){marker}")

        default_provider_idx = (
            available.index(current_provider) + 1 if current_provider in available else 1
        )
        provider_idx = _pick_number(
            f"Provider [{default_provider_idx}]: ",
            max_value=len(available),
            default_value=default_provider_idx,
        )
        selected_provider = available[provider_idx - 1]

    models = RUN_PROVIDER_MODELS.get(selected_provider, ["latest"])
    print(f"\nChoose model for {selected_provider}:")
    for idx, model in enumerate(models, start=1):
        marker = " (current)" if selected_provider == current_provider and model == current_model else ""
        print(f"  {idx}) {model}{marker}")

    default_model_idx = 1
    if selected_provider == current_provider and current_model in models:
        default_model_idx = models.index(current_model) + 1

    model_idx = _pick_number(
        f"Model [{default_model_idx}]: ",
        max_value=len(models),
        default_value=default_model_idx,
    )
    selected_model = models[model_idx - 1]

    print(f"\nUsing {selected_provider} ({selected_model}) for this run.")
    print("Tip: for custom model IDs, set LLM_MODEL directly in .env.")
    return selected_provider, selected_model


def _patched_env_example() -> str:
    template_path = PROJECT_ROOT / ".env.example"
    if template_path.exists():
        content = template_path.read_text(encoding="utf-8")
    else:
        content = (
            "LLM_PROVIDER=openai\n"
            "LLM_MODEL=latest\n"
            "OPENAI_API_KEY=\n"
            "DEEPSEEK_API_KEY=\n"
            "TELEGRAM_BOT_TOKEN=\n"
            f"MEMORY_DB_PATH={RUNTIME_DB}\n"
            f"WORKSPACE_PATH={RUNTIME_WORKSPACE}\n"
            "CONTEXT_WINDOW=128000\n"
        )

    if re.search(r"(?m)^MEMORY_DB_PATH=", content):
        content = re.sub(r"(?m)^MEMORY_DB_PATH=.*$", f"MEMORY_DB_PATH={RUNTIME_DB}", content)
    else:
        content += f"\nMEMORY_DB_PATH={RUNTIME_DB}\n"

    if re.search(r"(?m)^WORKSPACE_PATH=", content):
        content = re.sub(r"(?m)^WORKSPACE_PATH=.*$", f"WORKSPACE_PATH={RUNTIME_WORKSPACE}", content)
    else:
        content += f"\nWORKSPACE_PATH={RUNTIME_WORKSPACE}\n"

    return content


def _upgrade_legacy_env_defaults(content: str) -> tuple[str, bool]:
    """Upgrade legacy default paths to runtime defaults (non-custom values only)."""
    updated = content
    changed = False

    workspace_patterns = [
        r"(?m)^WORKSPACE_PATH\s*=\s*workspace\s*(?:#.*)?$",
        r"(?m)^WORKSPACE_PATH\s*=\s*\./workspace\s*(?:#.*)?$",
    ]
    memory_patterns = [
        r"(?m)^MEMORY_DB_PATH\s*=\s*lightclaw\.db\s*(?:#.*)?$",
        r"(?m)^MEMORY_DB_PATH\s*=\s*\./lightclaw\.db\s*(?:#.*)?$",
    ]

    for pattern in workspace_patterns:
        new_text, count = re.subn(pattern, f"WORKSPACE_PATH={RUNTIME_WORKSPACE}", updated)
        if count:
            changed = True
            updated = new_text

    for pattern in memory_patterns:
        new_text, count = re.subn(pattern, f"MEMORY_DB_PATH={RUNTIME_DB}", updated)
        if count:
            changed = True
            updated = new_text

    return updated, changed


def _migrate_legacy_workspace(home: Path, workspace_dir: Path) -> int:
    """Copy legacy home/workspace files into runtime workspace if missing."""
    legacy = home / "workspace"
    if not legacy.exists() or not legacy.is_dir():
        return 0

    migrated = 0
    for src in legacy.rglob("*"):
        if not src.is_file():
            continue
        rel = src.relative_to(legacy)
        dst = workspace_dir / rel
        if dst.exists():
            continue
        dst.parent.mkdir(parents=True, exist_ok=True)
        shutil.copy2(src, dst)
        migrated += 1

    return migrated


def _ensure_workspace_templates(workspace_dir: Path, force: bool) -> list[str]:
    created: list[str] = []
    source_workspace = PROJECT_ROOT / "templates" / "personality"
    if not source_workspace.exists():
        source_workspace = PROJECT_ROOT / "workspace"

    defaults = {
        "IDENTITY.md": "# LightClaw\n\nA focused personal AI assistant.",
        "SOUL.md": "# Soul\n\nBe helpful, accurate, and concise.",
        "USER.md": "# User\n\nPersonal preferences go here.",
    }

    for filename in TEMPLATE_FILES:
        target = workspace_dir / filename
        if target.exists() and not force:
            continue

        source = source_workspace / filename
        if source.exists():
            content = source.read_text(encoding="utf-8")
        else:
            content = defaults[filename]

        target.write_text(content, encoding="utf-8")
        created.append(filename)

    return created


def _confirm_reset_env(env_path: Path) -> bool:
    """Ask whether to reset .env when onboarding is re-run."""
    if not sys.stdin.isatty():
        return False
    print("")
    print(f"Existing config found: {env_path}")
    print("Do you want to reset .env from the latest template now?")
    answer = input("Reset .env? [y/N]: ").strip().lower()
    return answer in {"y", "yes"}


def _set_env_value(content: str, key: str, value: str) -> str:
    line = f"{key}={value}"
    pattern = rf"(?m)^{re.escape(key)}=.*$"
    if re.search(pattern, content):
        return re.sub(pattern, line, content)
    suffix = "" if content.endswith("\n") else "\n"
    return f"{content}{suffix}{line}\n"


def _get_env_value(content: str, key: str) -> str:
    match = re.search(rf"(?m)^{re.escape(key)}=(.*)$", content)
    if not match:
        return ""
    value = (match.group(1) or "").strip()
    if not value:
        return ""
    # Treat comment placeholders as empty and strip inline comments.
    if value.startswith("#"):
        return ""
    return re.sub(r"\s+#.*$", "", value).strip()


def _clear_other_provider_keys(content: str, keep_key: str) -> str:
    provider_keys = (
        "OPENAI_API_KEY",
        "XAI_API_KEY",
        "ANTHROPIC_API_KEY",
        "GEMINI_API_KEY",
        "DEEPSEEK_API_KEY",
        "ZAI_API_KEY",
    )
    updated = content
    for key in provider_keys:
        if key == keep_key:
            continue
        updated = _set_env_value(updated, key, "")
    return updated


def _read_secret(prompt: str) -> str:
    """Read secret input, with fallback for non-standard terminals."""
    try:
        return getpass.getpass(prompt).strip()
    except Exception:
        try:
            return input(prompt).strip()
        except EOFError:
            return ""


def _configure_env_interactive(
    env_path: Path,
    force_prompt: bool = False,
    fallback_content: str = "",
) -> bool:
    """Optional guided provider/token setup for .env."""
    if not sys.stdin.isatty():
        return False

    if not force_prompt:
        answer = input("Configure provider and tokens now? [Y/n]: ").strip().lower()
        if answer in {"n", "no"}:
            return False

    providers = [
        ("openai", "OpenAI", "OPENAI_API_KEY", "gpt-5.2"),
        ("xai", "xAI", "XAI_API_KEY", "grok-4-latest"),
        ("claude", "Anthropic", "ANTHROPIC_API_KEY", "claude-opus-4-5"),
        ("gemini", "Google", "GEMINI_API_KEY", "gemini-3-flash-preview"),
        ("deepseek", "DeepSeek", "DEEPSEEK_API_KEY", "deepseek-chat"),
        ("zai", "Z-AI", "ZAI_API_KEY", "glm-5"),
    ]

    print("")
    print("Choose provider:")
    for idx, (_, name, _, model) in enumerate(providers, start=1):
        print(f"  {idx}) {name} ({model})")

    content = env_path.read_text(encoding="utf-8")

    def existing_value(key: str) -> str:
        current = _get_env_value(content, key)
        if current:
            return current
        return _get_env_value(fallback_content, key) if fallback_content else ""

    existing_provider = existing_value("LLM_PROVIDER")
    default_provider_idx = None
    for idx, (provider_id, _, _, _) in enumerate(providers, start=1):
        if provider_id == existing_provider:
            default_provider_idx = idx
            break

    selected = None
    while selected is None:
        default_label = (
            f" [{default_provider_idx}]"
            if default_provider_idx is not None
            else f" [1-{len(providers)}]"
        )
        raw = input(f"Provider number{default_label}: ").strip()
        if not raw and default_provider_idx is not None:
            choice = default_provider_idx
        else:
            try:
                choice = int(raw)
            except ValueError:
                choice = 0
        if 1 <= choice <= len(providers):
            selected = providers[choice - 1]
        else:
            print(f"Invalid choice. Enter 1-{len(providers)}.")

    provider_id, provider_name, key_env, default_model = selected
    existing_model = existing_value("LLM_MODEL")
    model_default = (
        existing_model
        if existing_provider == provider_id and existing_model
        else default_model
    )
    model = input(f"Model [{model_default}]: ").strip() or model_default

    existing_key = existing_value(key_env)
    api_key = _read_secret(f"{key_env} (leave blank to keep current): ")
    if not api_key:
        api_key = existing_key

    existing_telegram_token = existing_value("TELEGRAM_BOT_TOKEN")
    telegram_token = input("TELEGRAM_BOT_TOKEN (leave blank to keep current): ").strip()
    if not telegram_token:
        telegram_token = existing_telegram_token

    existing_allowed_users = existing_value("TELEGRAM_ALLOWED_USERS")
    allowed_users = input(
        "TELEGRAM_ALLOWED_USERS (optional, comma-separated, blank keeps current): "
    ).strip()
    if not allowed_users:
        allowed_users = existing_allowed_users

    content = _set_env_value(content, "LLM_PROVIDER", provider_id)
    content = _set_env_value(content, "LLM_MODEL", model)
    content = _set_env_value(content, key_env, api_key)
    content = _clear_other_provider_keys(content, keep_key=key_env)
    content = _set_env_value(content, "TELEGRAM_BOT_TOKEN", telegram_token)
    content = _set_env_value(content, "TELEGRAM_ALLOWED_USERS", allowed_users)

    env_path.write_text(content, encoding="utf-8")
    print(f"- Guided config applied: provider={provider_name}, model={model}")
    return True


def cmd_onboard(args: argparse.Namespace) -> int:
    home = _resolve_home(args.home)
    runtime_dir = home / RUNTIME_DIRNAME
    workspace_dir = runtime_dir / "workspace"
    env_path = home / ".env"

    runtime_dir.mkdir(parents=True, exist_ok=True)
    workspace_dir.mkdir(parents=True, exist_ok=True)

    migrated = _migrate_legacy_workspace(home, workspace_dir)
    created_templates = _ensure_workspace_templates(workspace_dir, force=args.force)

    env_existed = env_path.exists()
    previous_env_content = env_path.read_text(encoding="utf-8") if env_existed else ""
    wrote_env = False
    upgraded_env = False
    reset_env = False
    if env_existed and not args.force and not args.reset_env:
        original = env_path.read_text(encoding="utf-8")
        upgraded, changed = _upgrade_legacy_env_defaults(original)
        if changed:
            env_path.write_text(upgraded, encoding="utf-8")
            upgraded_env = True
        elif _confirm_reset_env(env_path):
            env_path.write_text(_patched_env_example(), encoding="utf-8")
            wrote_env = True
            reset_env = True
    else:
        env_path.write_text(_patched_env_example(), encoding="utf-8")
        wrote_env = True
        reset_env = env_existed

    print("LightClaw onboard complete")
    print(f"- Home: {home}")
    print(f"- Runtime dir: {runtime_dir}")
    print(f"- Workspace: {workspace_dir}")
    print(f"- Database: {runtime_dir / 'lightclaw.db'}")
    if wrote_env and reset_env:
        print(f"- Reset config from template: {env_path}")
    elif wrote_env:
        print(f"- Wrote config: {env_path}")
    elif upgraded_env:
        print(f"- Upgraded legacy defaults in config: {env_path}")
    else:
        print(f"- Kept existing config: {env_path}")
    if created_templates:
        print(f"- Created templates: {', '.join(created_templates)}")
    else:
        print("- Template files already present")
    if migrated:
        print(f"- Migrated {migrated} legacy file(s) from ./workspace")

    configured = False
    fallback_content = previous_env_content if reset_env else ""
    if args.configure:
        configured = _configure_env_interactive(
            env_path,
            force_prompt=True,
            fallback_content=fallback_content,
        )
    elif wrote_env or reset_env:
        print("")
        configured = _configure_env_interactive(
            env_path,
            force_prompt=False,
            fallback_content=fallback_content,
        )

    print("")
    print("Next steps:")
    if configured:
        print("1. Review .env if needed.")
    else:
        print("1. Edit .env and set TELEGRAM_BOT_TOKEN + provider API key.")
    print("2. Run: lightclaw run")
    return 0


def _prepare_runtime_environment(
    home: Path,
    provider_override_raw: str,
    model_override_raw: str,
    prompt_selection: bool = True,
) -> int:
    """Prepare runtime directories, load env, and apply provider/model selection."""
    runtime_dir = home / RUNTIME_DIRNAME
    workspace_dir = runtime_dir / "workspace"

    runtime_dir.mkdir(parents=True, exist_ok=True)
    workspace_dir.mkdir(parents=True, exist_ok=True)
    _migrate_legacy_workspace(home, workspace_dir)
    _ensure_workspace_templates(workspace_dir, force=False)

    env_path = home / ".env"
    if env_path.exists():
        load_dotenv(env_path, override=False)

    current_provider = os.getenv("LLM_PROVIDER", "").strip().lower()
    current_model = os.getenv("LLM_MODEL", "").strip()
    provider_override = (provider_override_raw or "").strip().lower()
    model_override = (model_override_raw or "").strip()

    if provider_override and provider_override not in RUN_PROVIDER_KEY_ENV:
        supported = ", ".join(RUN_PROVIDER_KEY_ENV.keys())
        print(f"Unsupported provider '{provider_override}'. Supported: {supported}")
        return 2

    if provider_override:
        required_env_key = RUN_PROVIDER_KEY_ENV[provider_override]
        if not os.getenv(required_env_key, "").strip():
            print(
                f"Provider '{provider_override}' requires {required_env_key} in .env. "
                "Set it first, then retry."
            )
            return 2

        os.environ["LLM_PROVIDER"] = provider_override
        if model_override:
            os.environ["LLM_MODEL"] = model_override
        elif provider_override != current_provider or not current_model:
            os.environ["LLM_MODEL"] = RUN_PROVIDER_MODELS.get(provider_override, ["latest"])[0]

    elif model_override:
        os.environ["LLM_MODEL"] = model_override

    elif prompt_selection:
        selected_provider, selected_model = _prompt_run_provider_model(
            current_provider=current_provider,
            current_model=current_model,
        )
        if selected_provider:
            os.environ["LLM_PROVIDER"] = selected_provider
            os.environ["LLM_MODEL"] = selected_model

    os.environ["LIGHTCLAW_HOME"] = str(home)
    workspace_env = os.getenv("WORKSPACE_PATH", "").strip()
    memory_env = os.getenv("MEMORY_DB_PATH", "").strip()
    if not workspace_env or workspace_env in {"workspace", "./workspace"}:
        os.environ["WORKSPACE_PATH"] = RUNTIME_WORKSPACE
    if not memory_env or memory_env in {"lightclaw.db", "./lightclaw.db"}:
        os.environ["MEMORY_DB_PATH"] = RUNTIME_DB

    os.chdir(home)
    return 0


def cmd_run(args: argparse.Namespace) -> int:
    home = _resolve_home(args.home)
    prep_code = _prepare_runtime_environment(
        home,
        args.provider or "",
        args.model or "",
        prompt_selection=True,
    )
    if prep_code:
        return prep_code

    from main import main as bot_main

    bot_main()
    return 0


def cmd_chat(args: argparse.Namespace) -> int:
    home = _resolve_home(args.home)
    prep_code = _prepare_runtime_environment(
        home,
        args.provider or "",
        args.model or "",
        prompt_selection=False,
    )
    if prep_code:
        return prep_code

    os.environ["LIGHTCLAW_CHAT_MODE"] = "1"

    from config import load_config
    from main import LightClawBot, build_system_prompt, resolve_runtime_path

    # Keep terminal chat clean: suppress noisy transport/client logs.
    logging.getLogger("httpx").setLevel(logging.WARNING)
    logging.getLogger("httpcore").setLevel(logging.WARNING)
    logging.getLogger("openai._base_client").setLevel(logging.WARNING)

    config = load_config()
    config.workspace_path = str(resolve_runtime_path(config.workspace_path))
    config.memory_db_path = str(resolve_runtime_path(config.memory_db_path))
    config.skills_state_path = str(resolve_runtime_path(config.skills_state_path))

    workspace = Path(config.workspace_path)
    workspace.mkdir(parents=True, exist_ok=True)

    if not config.llm_provider:
        print("No provider configured. Set LLM_PROVIDER + API key in .env.")
        return 2

    bot = LightClawBot(config)
    session_id = (args.session or "cli").strip() or "cli"
    cli_user_id = (
        (config.telegram_allowed_users[0] if config.telegram_allowed_users else "cli-user")
    )

    print("")
    print(f"ü¶û LightClaw terminal chat")
    print(f"Provider: {config.llm_provider} ({config.llm_model})")
    print(f"Session: {session_id}")
    print("Type /help for commands, /exit to quit.")
    print("")

    async def _chat_once(user_text: str) -> str:
        if bot._llm_backoff_active():
            remaining = bot._llm_backoff_remaining_sec()
            wait_hint = f"{remaining}s" if remaining > 0 else "a short while"
            bot.memory.ingest("user", user_text, session_id)
            return (
                f"‚ö†Ô∏è {config.llm_provider} is temporarily unavailable "
                "(quota/billing or rate limit).\n"
                f"Please retry in about {wait_hint}, or top up your provider balance."
            )

        memories = bot.memory.recall(user_text, top_k=config.memory_top_k)
        memories = bot._filter_recalled_memories(memories)
        memories_text = bot.memory.format_memories_for_prompt(memories)

        recent = bot.memory.get_recent(session_id, limit=20)
        recent = bot._clean_orphan_messages(recent)
        recent = bot._filter_recent_context(recent)

        summary = bot._get_session_summary(session_id)
        skills_text = await asyncio.to_thread(bot.skills.prompt_context, session_id)

        system_prompt = build_system_prompt(
            config, bot.personality, memories_text, summary, skills_text
        )
        messages = list(recent)
        messages.append({"role": "user", "content": user_text})

        response = None
        max_retries = 2
        for retry in range(max_retries + 1):
            try:
                response = await bot.llm.chat(messages, system_prompt)
                break
            except Exception as e:
                if retry < max_retries and bot._is_context_error(str(e)) and len(messages) > 4:
                    mid = len(messages) // 2
                    messages = (
                        messages[:1]
                        + [{"role": "system", "content": "[Emergency context compression]"}]
                        + messages[mid:]
                    )
                    continue
                response = f"‚ö†Ô∏è Error communicating with {config.llm_provider}: {e}"
                break

        if response is None:
            response = "‚ö†Ô∏è Failed to get a response after retries. Please try again."

        provider_error_response = bot._is_provider_error_text(response)
        if provider_error_response:
            bot._set_llm_backoff()
        else:
            bot._clear_llm_backoff()

        bot.memory.ingest("user", user_text, session_id)

        file_ops, cleaned_response = await bot._process_file_blocks(response)
        repair_ops = await bot._repair_incomplete_html(session_id, user_text, file_ops)
        if repair_ops:
            repaired_paths = {op.path for op in repair_ops if op.action != "error"}
            if repaired_paths:
                file_ops = [op for op in file_ops if op.path not in repaired_paths]
            file_ops.extend(repair_ops)

        success_ops = [op for op in file_ops if op.action != "error" and op.path]
        if success_ops:
            bot._last_file_by_session[session_id] = success_ops[-1].path

        workspace_label = bot._workspace_display_path()
        visible_response = cleaned_response
        if file_ops:
            success_count = sum(1 for op in file_ops if op.action != "error")
            if success_count > 0:
                visible_response = "Done. Saved requested changes to files."
            else:
                visible_response = bot._compact_response_for_file_ops(cleaned_response)

        response_parts = [visible_response] if visible_response else []
        if file_ops:
            response_parts.append(
                bot._render_file_operations(
                    file_ops,
                    include_diffs=False,
                    workspace_label=workspace_label,
                )
            )
        final_text = "\n\n".join(part for part in response_parts if part).strip() or "Done."

        memory_text = visible_response if file_ops else cleaned_response
        memory_parts = [memory_text] if memory_text else []
        if file_ops:
            memory_parts.append(
                bot._render_file_operations(
                    file_ops,
                    include_diffs=False,
                    workspace_label=workspace_label,
                )
            )
        memory_response = "\n\n".join(part for part in memory_parts if part).strip() or "Done."
        bot.memory.ingest("assistant", memory_response, session_id)

        if not provider_error_response:
            await bot.maybe_summarize(session_id)
        return final_text

    def _render_terminal_reply(text: str, parse_mode: str | None = None):
        rendered = bot._strip_html_for_log(text) if parse_mode else text
        print(f"bot> {rendered}\n")

    class _CliSentMessage:
        def __init__(self):
            self.text = ""

        async def edit_text(self, text: str, parse_mode: str | None = None):
            _render_terminal_reply(text, parse_mode=parse_mode)
            self.text = text
            return self

    class _CliIncomingMessage:
        def __init__(self, text: str):
            self.text = text

        async def reply_text(self, text: str, parse_mode: str | None = None):
            _render_terminal_reply(text, parse_mode=parse_mode)
            sent = _CliSentMessage()
            sent.text = text
            return sent

    class _CliBotAPI:
        async def send_chat_action(self, chat_id, action):
            return None

    class _CliContext:
        def __init__(self, args_list: list[str]):
            self.args = args_list
            self.bot = _CliBotAPI()

    async def _run_terminal_command(raw_line: str) -> bool:
        if raw_line.lower() in {"/exit", "/quit"}:
            print("bye.")
            return True

        try:
            parts = shlex.split(raw_line[1:])
        except ValueError as e:
            print(f"bot> ‚ö†Ô∏è Invalid command syntax: {e}\n")
            return False

        if not parts:
            return False

        command = parts[0].lower()
        args_list = parts[1:]

        handler_map = {
            "start": bot.cmd_start,
            "help": bot.cmd_help,
            "clear": bot.cmd_clear,
            "wipe_memory": bot.cmd_wipe_memory,
            "wipe": bot.cmd_wipe_memory,
            "memory": bot.cmd_memory,
            "recall": bot.cmd_recall,
            "skills": bot.cmd_skills,
            "agent": bot.cmd_agent,
            "heartbeat": bot.cmd_heartbeat,
            "show": bot.cmd_show,
        }
        handler = handler_map.get(command)
        if not handler:
            print("bot> Unknown command. Type /help for available commands.\n")
            return False

        class _CliUpdate:
            def __init__(self, line_text: str):
                self.effective_user = type("U", (), {"id": cli_user_id})()
                self.effective_chat = type("C", (), {"id": session_id})()
                self.message = _CliIncomingMessage(line_text)

        update = _CliUpdate(raw_line)
        context = _CliContext(args_list)
        await handler(update, context)
        return False

    while True:
        try:
            user_text = input("you> ").strip()
        except (EOFError, KeyboardInterrupt):
            print("\nbye.")
            return 0

        if not user_text:
            continue

        if user_text.startswith("/"):
            try:
                should_exit = asyncio.run(_run_terminal_command(user_text))
            except Exception as e:
                print(f"bot> ‚ö†Ô∏è Command failed: {e}\n")
                should_exit = False
            if should_exit:
                return 0
            continue

        try:
            reply = asyncio.run(_chat_once(user_text))
        except Exception as e:
            reply = f"‚ö†Ô∏è Internal error: {e}"
        print(f"bot> {reply}\n")


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(prog="lightclaw", description="LightClaw CLI")
    sub = parser.add_subparsers(dest="command", required=True)

    onboard = sub.add_parser("onboard", help="Initialize .env and runtime workspace in current directory")
    onboard.add_argument("--home", help="Target runtime home directory (default: current directory)")
    onboard.add_argument(
        "--force",
        action="store_true",
        help="Overwrite existing .env and workspace templates",
    )
    onboard.add_argument(
        "--reset-env",
        action="store_true",
        help="Reset existing .env from template without overwriting workspace templates",
    )
    onboard.add_argument(
        "--configure",
        action="store_true",
        help="Run guided provider/token configuration using current .env",
    )
    onboard.set_defaults(func=cmd_onboard)

    run = sub.add_parser(
        "run",
        help="Run LightClaw using current directory as runtime home",
        description=(
            "Run LightClaw.\n"
            "If --provider/--model are omitted and a TTY is attached, "
            "you can choose provider/model interactively from configured API keys."
        ),
    )
    run.add_argument("--home", help="Runtime home directory (default: current directory)")
    run.add_argument(
        "--provider",
        help="Override provider for this run (openai|xai|claude|gemini|deepseek|zai)",
    )
    run.add_argument(
        "--model",
        help="Override model for this run (used with current/overridden provider)",
    )
    run.set_defaults(func=cmd_run)

    chat = sub.add_parser(
        "chat",
        help="Run a local terminal chat session",
        description=(
            "Run terminal chat with LightClaw.\n"
            "Uses the same memory DB/workspace/provider stack as Telegram."
        ),
    )
    chat.add_argument("--home", help="Runtime home directory (default: current directory)")
    chat.add_argument(
        "--provider",
        help="Override provider for this session (openai|xai|claude|gemini|deepseek|zai)",
    )
    chat.add_argument(
        "--model",
        help="Override model for this session (used with current/overridden provider)",
    )
    chat.add_argument(
        "--session",
        default="cli",
        help="Session id for terminal memory (default: cli)",
    )
    chat.set_defaults(func=cmd_chat)

    return parser


def main() -> int:
    parser = build_parser()
    args = parser.parse_args()
    return args.func(args)


if __name__ == "__main__":
    raise SystemExit(main())
