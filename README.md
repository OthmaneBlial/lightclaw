<div align="center">

  <h1>ü¶û LightClaw</h1>

  <h3>The Featherweight Core of OpenClaw ‚Äî Your AI Agent in a Tiny Codebase</h3>

  <p><strong>OpenClaw-inspired Python AI agent</strong> for Telegram, long-term memory, and multi-provider LLM support.</p>

  <p>
    <img src="https://img.shields.io/badge/Python-3.10+-3776AB?style=flat&logo=python&logoColor=white" alt="Python">
    <img src="https://img.shields.io/badge/Core-lean-brightgreen" alt="Core">
    <img src="https://img.shields.io/badge/Repo-~5k_lines-blue" alt="Lines">
    <img src="https://img.shields.io/badge/LLM_Providers-5-purple" alt="Providers">
    <img src="https://img.shields.io/badge/RAM-<50MB-orange" alt="RAM">
    <img src="https://img.shields.io/badge/license-MIT-green" alt="License">
  </p>

  <p><i>Fork it. Hack it. Ship it. No framework tax.</i></p>

</div>

---

## Why LightClaw Exists

**OpenClaw** is a powerful, full-featured AI agent platform ‚Äî but it's also *big*. Dozens of packages, multiple channels, tool registries, message buses, plugin systems. It's built for scale and enterprise use.

**LightClaw** is the opposite. It's the *distilled essence* of the OpenClaw idea, stripped down to the atomic minimum:

If you are searching for an **OpenClaw alternative**, **OpenClaw in Python**, or a **self-hosted Telegram AI assistant with memory**, this repository is built for that exact use case.

```
OpenClaw:     50+ packages ‚îÇ 20k+ lines  ‚îÇ TypeScript ‚îÇ 10+ channels ‚îÇ 12+ providers ‚îÇ >1GB RAM
LightClaw:    5 files       ‚îÇ ~1300 lines ‚îÇ Python ‚îÇ Telegram only ‚îÇ 6 providers ‚îÇ <50MB RAM
```

Think of LightClaw as **the starter engine** ‚Äî the part of a rocket that ignites first. It contains the core DNA of OpenClaw (LLM routing, memory, conversational agent) but removes everything else. No message bus. No plugin registry. No tool orchestration. Just a direct pipeline:

```
üì± Telegram Message ‚Üí üß† Memory Recall ‚Üí ü§ñ LLM ‚Üí üí° HTML Format ‚Üí üí¨ Reply
```

## Looking for OpenClaw?

- OpenClaw GitHub: https://github.com/openclaw/openclaw
- OpenClaw docs: https://docs.openclaw.ai/
- LightClaw focuses on the lightweight Python path: Telegram-first, memory-enabled, and easy to fork.

## Who Is This For?

<table>
  <tr>
    <td>üßë‚Äçüíª <b>Builders</b></td>
    <td>You want to build <i>your own</i> AI assistant without inheriting a massive codebase. Fork LightClaw, add what you need, nothing more.</td>
  </tr>
  <tr>
    <td>üéì <b>Learners</b></td>
    <td>You want to understand how AI agents work ‚Äî memory, RAG, LLM routing ‚Äî in code you can read in 30 minutes.</td>
  </tr>
  <tr>
    <td>‚ö° <b>Minimalists</b></td>
    <td>You need a personal AI bot on a $5/month VPS. No Docker. No build steps. Just <code>./lightclaw run</code>.</td>
  </tr>
  <tr>
    <td>üî¨ <b>Tinkerers</b></td>
    <td>You want to experiment with different LLM providers, memory strategies, or prompt engineering without fighting a framework.</td>
  </tr>
</table>

## The Core Idea

> **OpenClaw is the Industrial Complex. LightClaw is the Precision Workbench.**
>
> You don't need an entire industrial complex to build a custom tool. You need a workbench with the right instruments. LightClaw gives you exactly that ‚Äî a clean, readable, forkable foundation that does one thing well: **connect you to an AI through Telegram, with infinite memory.**
>
> Add Discord support? Drop in a file. Need tool calling? Add a function. Want vector search with FAISS? Swap out 20 lines in `memory.py`. The codebase is small enough that *you own it completely*.

## Features

üß† **Infinite Memory** ‚Äî Every conversation is persisted in SQLite with TF-IDF vector embeddings. The bot recalls relevant context from days, weeks, or months ago via semantic search (RAG).

üîå **6 LLM Providers** ‚Äî OpenAI (ChatGPT), xAI (Grok), Anthropic (Claude), Google (Gemini), DeepSeek, Z-AI (GLM). Switch providers by changing one line in `.env`.

üì± **Telegram Native** ‚Äî Polling-based bot with "Thinking‚Ä¶ üí≠" placeholders, HTML-formatted responses, typing indicators, and rich commands.

üé≠ **Customizable Personality** ‚Äî Edit `.lightclaw/workspace/SOUL.md`, `IDENTITY.md`, and `USER.md` to shape your bot's character, identity, and personal context.

üß© **Skill System (ClawHub + Local)** ‚Äî Install skills from `clawhub.ai`, activate them per chat with `/skills`, and create your own custom skills locally.

ü§ñ **Local Agent Delegation** ‚Äî Delegate large build tasks to installed local coding agents (`codex`, `claude`, `opencode`) with `/agent`, while LightClaw reports workspace change summaries back in Telegram.

üõ†Ô∏è **Workspace File Operations + Diff Summaries** ‚Äî Large code is written directly to `.lightclaw/workspace` (not dumped in chat). LightClaw applies create/edit operations, then returns concise operation + diff line summaries.

üß± **Truncation Recovery for Large Files** ‚Äî If an LLM response is cut mid-file, LightClaw attempts continuation/repair passes (including HTML completion) before finalizing the saved file.

üéôÔ∏è **Voice Messages** ‚Äî Automatic voice transcription via Groq Whisper (optional). Send a voice note and the bot transcribes + responds.

üì∏ **Photo & Document Support** ‚Äî Send images and files ‚Äî the bot acknowledges them and processes captions through the agent loop.

üßπ **Smart Context Management** ‚Äî Auto-summarization when conversations grow too long, plus emergency context window compression with retry on overflow.

üì¶ **Small Core** ‚Äî `main.py` + `memory.py` + `providers.py` + `config.py` + `lightclaw` CLI. No hidden complexity. No abstractions for the sake of abstractions.

üöÄ **Instant Startup** ‚Äî No compilation, no Docker, no build pipeline. `./lightclaw run` and you're running.

## Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      main.py                                      ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                     ‚îÇ
‚îÇ  ‚îÇ markdown_to_telegram_html()             ‚îÇ  MD ‚Üí HTML converter‚îÇ
‚îÇ  ‚îÇ load_personality()                      ‚îÇ  .lightclaw/workspace/*.md ‚îÇ
‚îÇ  ‚îÇ build_system_prompt()                   ‚îÇ  Dynamic prompts    ‚îÇ
‚îÇ  ‚îÇ transcribe_voice()                      ‚îÇ  Groq Whisper       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  LightClawBot                                                    ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ handle_message()    ‚Üê text messages                         ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ handle_voice()      ‚Üê voice transcription                   ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ handle_photo()      ‚Üê image handling                        ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ handle_document()   ‚Üê file handling                         ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ _process_user_message()                                     ‚îÇ
‚îÇ  ‚îÇ     ‚îÇ                                                         ‚îÇ
‚îÇ  ‚îÇ     ‚îú‚îÄ 1. Send "Thinking‚Ä¶ üí≠"  placeholder                   ‚îÇ
‚îÇ  ‚îÇ     ‚îú‚îÄ 2. Recall memories      ‚óÑ‚îÄ‚îÄ memory.py                  ‚îÇ
‚îÇ  ‚îÇ     ‚îú‚îÄ 3. Build prompt              SQLite + TF-IDF RAG      ‚îÇ
‚îÇ  ‚îÇ     ‚îú‚îÄ 4. Call LLM + retry     ‚óÑ‚îÄ‚îÄ providers.py               ‚îÇ
‚îÇ  ‚îÇ     ‚îú‚îÄ 5. Edit placeholder          6 providers unified       ‚îÇ
‚îÇ  ‚îÇ     ‚îî‚îÄ 6. Summarize if needed                                 ‚îÇ
‚îÇ  ‚îÇ                                                               ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ cmd_start/help/clear/wipe_memory/memory/recall/skills/agent/show ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  config.py ‚óÑ‚îÄ‚îÄ .env file                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Quick Start

### ‚ö° One-Command Setup (Recommended)

```bash
git clone https://github.com/OthmaneBlial/lightclaw.git && cd lightclaw && bash setup.sh
```

The interactive setup wizard will walk you through:
1. Choosing your AI provider (OpenAI, xAI, Claude, Gemini, DeepSeek, Z-AI)
2. Entering your API key
3. Creating a Telegram bot via @BotFather (step-by-step guide)
4. Optional voice transcription setup
5. Auto-start your bot üöÄ

### üîß Manual Setup

```bash
git clone https://github.com/OthmaneBlial/lightclaw.git
cd lightclaw
pip install -r requirements.txt
```

**2. Onboard (recommended)**

```bash
./lightclaw onboard
```

This creates:
- `.env` (if missing)
- `.lightclaw/workspace/` (runtime personality files)
- `.lightclaw/lightclaw.db` (runtime DB path)

Then edit `.env` with your API key and Telegram bot token:

```env
# Choose your provider: openai | xai | claude | gemini | deepseek | zai
LLM_PROVIDER=openai
LLM_MODEL=latest
OPENAI_API_KEY=sk-...
DEEPSEEK_API_KEY=

# Get a token from @BotFather on Telegram
TELEGRAM_BOT_TOKEN=123456:ABC...

# Optional: restrict to your user ID (get it from @userinfobot)
TELEGRAM_ALLOWED_USERS=123456789

# Optional tuning for large code/file generation
MAX_OUTPUT_TOKENS=12000
LOCAL_AGENT_TIMEOUT_SEC=1800

# Skills (default registry)
SKILLS_HUB_BASE_URL=https://clawhub.ai
SKILLS_STATE_PATH=.lightclaw/skills_state.json
```

**3. Customize (Optional)**

Edit the personality files in `.lightclaw/workspace/`:

```
.lightclaw/workspace/
‚îú‚îÄ‚îÄ IDENTITY.md   # Bot's name, purpose, philosophy
‚îú‚îÄ‚îÄ SOUL.md       # Personality traits and values
‚îî‚îÄ‚îÄ USER.md       # Your preferences and personal context
```

**4. Run**

```bash
./lightclaw run
```

That's it. Open Telegram, find your bot, say hello. ü¶û

> Development mode still works with `python main.py` (it now defaults to `.lightclaw/workspace`).

## CLI Commands

```bash
lightclaw onboard   # initialize .env + .lightclaw/workspace in current directory
lightclaw onboard --reset-env  # reset existing .env from latest template
lightclaw onboard --configure  # guided provider/model/key setup on current .env
lightclaw run       # run using the current directory as runtime home
lightclaw run --provider deepseek --model deepseek-chat  # one-run provider/model override
lightclaw chat      # local terminal chat mode (same memory/workspace/provider stack)
```

`lightclaw chat` supports the same core slash commands as Telegram (`/help`, `/skills`, `/agent`, `/show`, etc.).

If `lightclaw` is not on your `PATH`, run `./lightclaw onboard` and `./lightclaw run`.

## Supported Providers

| Provider | SDK Used | Set in `.env` | Model Examples |
|----------|----------|---------------|----------------|
| **OpenAI** | `openai` | `LLM_PROVIDER=openai` | `gpt-5.2`, `gpt-5.2-mini` |
| **xAI** | `openai` (base_url override) | `LLM_PROVIDER=xai` | `grok-4-latest`, `grok-4-fast-non-reasoning` |
| **Claude** | `anthropic` | `LLM_PROVIDER=claude` | `claude-opus-4-5`, `claude-sonnet-4-5` |
| **Gemini** | `google-generativeai` | `LLM_PROVIDER=gemini` | `gemini-3-flash-preview`, `gemini-2.5-flash` |
| **DeepSeek** | `openai` (base_url override) | `LLM_PROVIDER=deepseek` | `deepseek-chat`, `deepseek-reasoner` |
| **Z-AI** | `openai` (base_url override) | `LLM_PROVIDER=zai` | `glm-5`, `glm-4.7` |

> **Pro tip:** If `LLM_MODEL` is empty, `latest`, `auto`, or `default`, LightClaw picks the latest per-provider default automatically.

Quick provider sanity test:

```bash
python scripts/provider_smoke_test.py
```

It sends a tiny prompt to each provider with a configured API key and reports `OK`/`FAIL`/`SKIP`.

## Skills (ClawHub + Local)

Install and use skills directly from Telegram:

```text
/skills search sonos
/skills add sonoscli
/skills use sonoscli
/skills off sonoscli
/skills create my_custom_skill "My private workflow"
/skills show sonoscli
```

Runtime skill paths:
- Hub skills: `.lightclaw/workspace/skills/hub/<slug>/SKILL.md`
- Local skills: `.lightclaw/workspace/skills/local/<name>/SKILL.md`

Active skills are persisted per chat in `.lightclaw/skills_state.json`.

## Local Agent Delegation

Use local coding agents for bigger project work while keeping LightClaw as the single Telegram interface:

```text
/agent
/agent use codex
/agent codex Build a complete SaaS landing page with pricing + FAQ
/agent run Build a full React dashboard in this workspace
/agent run claude Add auth + routing to the current project
/agent off
```

Supported local agents (auto-detected from `PATH`): `codex`, `claude`, `opencode`.
You should authenticate these CLIs once on the host machine before using delegation mode.

How it behaves:
- `use` enables per-chat delegation mode (normal text messages are routed to that local agent).
- `run` executes one explicit delegated task.
- After each run, LightClaw reports a compact workspace delta (created/updated/deleted files).

## Run-Time Provider Selection

When you run `lightclaw run` in a terminal, LightClaw can prompt you to choose provider + model from configured keys in `.env` (numbered choices only).

For explicit non-interactive startup, use:

```bash
lightclaw run --provider deepseek --model deepseek-chat
```

If you want a custom model ID outside the preset menu, set `LLM_MODEL` directly in `.env`.

## Workspace Code Generation & Editing

LightClaw's file pipeline is optimized for coding-heavy chats:

- Saves generated artifacts to `.lightclaw/workspace` by default.
- Accepts full-file fenced blocks, filename fences, and explicit edit hunks.
- Applies edits with exact `SEARCH/REPLACE` semantics.
- Retries failed edit hunks with current file context.
- Avoids dumping large code blocks in Telegram responses.

Supported block styles include:

````text
```html:landing/index.html
<full file content>
```
````

````text
```edit:landing/index.html
<<<<<<< SEARCH
old text
=======
new text
>>>>>>> REPLACE
```
````

## Bot Commands

| Command | Description |
|---------|-------------|
| `/start` | Welcome message |
| `/help` | Show available commands |
| `/memory` | Show memory statistics (total interactions, sessions, vocabulary) |
| `/recall <query>` | Search past conversations by semantic similarity |
| `/skills` | List/search/install/activate/create/remove skills |
| `/agent` | Delegate tasks to local coding agents (`codex`, `claude`, `opencode`) |
| `/clear` | Reset conversation history for the current chat |
| `/wipe_memory` | Wipe all saved memory (requires explicit confirmation) |
| `/wipe` | Alias of `/wipe_memory` |
| `/show` | Show current model, provider, uptime, memory stats, voice status |

## How Infinite Memory Works

Unlike traditional chatbots that forget after a session ends, LightClaw stores **every interaction** in a local SQLite database with vector embeddings:

```
User says: "I love Italian food, especially pasta"
                    ‚îÇ
                    ‚ñº
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ  1. Tokenize     ‚îÇ  "love", "italian", "food", "pasta"
          ‚îÇ  2. TF-IDF Vec   ‚îÇ  [0.0, 0.3, 0.5, 0.7, ...]
          ‚îÇ  3. Store in DB   ‚îÇ  SQLite: content + embedding blob
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

... 3 weeks later ...

User says: "What food do I like?"
                    ‚îÇ
                    ‚ñº
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ  1. Embed query  ‚îÇ  [0.0, 0.2, 0.6, 0.0, ...]
          ‚îÇ  2. Cosine sim.  ‚îÇ  Compare with all stored vectors
          ‚îÇ  3. Top-K recall ‚îÇ  "I love Italian food" ‚Üí 0.82 sim
          ‚îÇ  4. Inject prompt‚îÇ  System prompt gets memory context
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
                    ‚ñº
          LLM responds: "You mentioned you love Italian food,
                         especially pasta! üçù"
```

## Smart Context Management

LightClaw automatically manages conversation length so you never hit context window limits:

1. **Auto-summarization** ‚Äî When history exceeds 20 messages or 75% of the context window, the LLM summarizes older messages while keeping the last 4 for continuity.
2. **Emergency compression** ‚Äî If the LLM returns a context-too-long error, LightClaw drops the oldest 50% of messages and retries automatically.
3. **Token estimation** ‚Äî Uses a 2.5 chars/token heuristic to predict when to summarize before hitting limits.
4. **Large output handling** ‚Äî Uses `MAX_OUTPUT_TOKENS` and file-save pipelines to keep long code generations reliable.

## Project Structure

```
lightclaw/
‚îú‚îÄ‚îÄ lightclaw         # CLI entrypoint: onboard + run
‚îú‚îÄ‚îÄ setup.sh          # One-command interactive setup wizard
‚îú‚îÄ‚îÄ main.py           # Telegram bot + agent loop + HTML converter
‚îú‚îÄ‚îÄ skills.py         # Skills manager (ClawHub + local + per-chat activation)
‚îú‚îÄ‚îÄ memory.py         # SQLite infinite memory + RAG
‚îú‚îÄ‚îÄ providers.py      # Unified LLM client for 6 providers
‚îú‚îÄ‚îÄ config.py         # .env configuration
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ provider_smoke_test.py  # Quick API smoke test for all providers
‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îî‚îÄ‚îÄ personality/  # Onboarding templates (IDENTITY.md, SOUL.md, USER.md)
‚îú‚îÄ‚îÄ .lightclaw/       # Runtime data (created by `lightclaw onboard`)
‚îÇ   ‚îú‚îÄ‚îÄ workspace/    # Active personality files + generated artifacts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ skills/   # Installed hub skills + local custom skills
‚îÇ   ‚îú‚îÄ‚îÄ lightclaw.db  # Runtime memory database
‚îÇ   ‚îî‚îÄ‚îÄ skills_state.json # Per-chat active skills state
‚îú‚îÄ‚îÄ requirements.txt  # 6 dependencies
‚îú‚îÄ‚îÄ .env.example      # Configuration template
‚îú‚îÄ‚îÄ LICENSE           # MIT
‚îî‚îÄ‚îÄ .gitignore
```

That's the entire project. No `src/`. No `pkg/`. No `internal/`.

## Fork & Build Your Own

LightClaw is designed to be forked. Here are some ideas:

| What You Want | What to Change |
|---------------|----------------|
| Add Discord support | Add a Discord handler in `main.py` (~50 lines) |
| Better embeddings | Swap TF-IDF in `memory.py` for `sentence-transformers` or OpenAI embeddings |
| Tool calling | Add tool definitions to `providers.py` and a tool executor in `main.py` |
| Web search | Add a search function and inject results into the prompt |
| Multi-user personas | Extend `.lightclaw/workspace/` with per-user personality files |
| Webhook mode | Replace polling with `python-telegram-bot`'s webhook handler |
| Vision support | Send photos to GPT-5.2 or GPT-4.1 vision APIs in `handle_photo()` |

The point is: **you shouldn't need permission from a framework to add a feature**. The code is small enough to understand in an afternoon and modify with confidence.

## OpenClaw Family

| Project | Language | Purpose | Complexity |
|---------|----------|---------|------------|
| **[OpenClaw](https://github.com/openclaw/openclaw)** | TypeScript | Full-featured AI agent platform | ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë |
| **[LightClaw](https://github.com/OthmaneBlial/lightclaw)** | Python | Minimal forkable agent core (6 LLMs) | ‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë |

> **LightClaw** is where you start. **OpenClaw** is where you scale.

## Requirements

- Python 3.10+
- A Telegram bot token ([get one from @BotFather](https://t.me/BotFather))
- An API key from any supported LLM provider
- (Optional) Groq API key for voice transcription

## License

MIT ‚Äî do whatever you want with it.

---

<div align="center">
  <p><b>ü¶û LightClaw ‚Äî Because the best framework is no framework.</b></p>
  <p><i>Built with ‚ù§Ô∏è by <a href="https://github.com/OthmaneBlial">Othmane BLIAL</a></i></p>
</div>
